# AI Intro


The introduction of the iPhone was the final spark needed to set off the mobile revolution. Ten years on, we are seeing emerging trends that coalesce into new paths for technology that will help define the decade that follows.

Thereâ€™s more data flowing around us and created by us than ever before, and old mechanisms to make sense of this data are struggling to keep up. Simple rule-based systems that used to be fine to generate activity feeds, separate acceptable content from that which is not allowed, or to analyze trends are reaching their natural limits of efficiency.

Today, we regularly interact with robots that can clean (e.g., Roomba) or entertain (e.g., Cozmo). Universities and companies around the world have advanced prototypes in place that include everything from self-driving cars to autonomous combat robots.

At the same time, the influx of new devices and exponential growth of data is matched on the cloud by the growth of increasingly sophisticated infrastructures at very large scale that allows companies to experiments on datasets and problems of a size and type that could not be attempted before.

The experiments are quickly turning into real-world solutions, allowing companies to provide new levels of functionality based on Artificial Intelligence (AI from now on) systems and algorithms that can be applied more broadly than they have ever been.

It is not surprising, then, that Machine Learning (ML from now on) is experiencing a technological renaissance, along with the superset of technologies we generally refer to as AI.

As part of this shift there has been a burst of new projects, systems, and services in all sizes. In particular, service infrastructure that can be deployed internally or used on-demand on the cloud to solve these problems.

Al encompasses many "subfields", ranging from the general, like vision, learning and perception, to the specific, such as playing complex board games like Chess or Go, proving mathematical theorems, writing music, or diagnosing diseases.

## AI What?

On Geoff Hinton's talk at 41:30 (https://youtu.be/VhmE_UXDOGs?t=2489).
Q: I just wanted to see what was on the slide about classical AI that you flashed.
A: Oh, too late now you missed it (laughs). I can tell you in one sentence. There's two paradigms for intelligence: one is logic, and in classical AI they thought that if you want to carry implications from one sentence to the next, you better do that by having a rule of inference, and binding symbols to the variables in this rule of inference, and letting the rule of inference carry the implication. And I want to make the analogy that physicists used to think that if you want light going from one place to another, you better have a medium that it disturbs... and it wasn't they thought that was a theory, 'that's just how it must be', the question is 'what is this medium, but it HAS to be like that', and for classical AI people they didn't think it was a theory, Newell and Simon said it was a theory, but the rest of them thought it was obviously true, that you have to have rules of inference to carry implications. What's happening in these thought vectors, is, you can carry implications from one sentence (centers?) to the next, using linear algebra, and there's nothing that looks like a rule of inference anywhere, and that's the end of classical AI.
